{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hw2.metrictool import MetricRegressionManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREPROCESSING = 'hw1\\df_preprocessing.pkl'\n",
    "\n",
    "TWEET = 'tweet'\n",
    "TWEET_CLEAN = 'clean_tweet'\n",
    "TWEET_TOKENIZE = 'tweet_token'\n",
    "TWEET_FILTERED = 'tweet_token_filtered'\n",
    "TWEET_STEMMED = 'tweet_stemmed'\n",
    "TWEET_LEMMATIZED = 'tweet_lemmatized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_manager = MetricRegressionManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thank, lyft, credit, use, cause, offer, wheel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love you take with you all the time in ur</td>\n",
       "      <td>[model, love, you, take, with, you, all, the, ...</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "2                                bihday your majesty   \n",
       "3    model love you take with you all the time in ur   \n",
       "4                  factsguide society now motivation   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, love, you, take, with, you, all, the, ...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                  [factsguide, society, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                  [bihday, majesti]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                        [factsguid, societi, motiv]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thank, lyft, credit, use, cause, offer, wheel...  \n",
       "2                                  [bihday, majesty]  \n",
       "3                      [model, love, take, time, ur]  \n",
       "4                  [factsguide, society, motivation]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df:pd.DataFrame = pd.read_pickle(PATH_PREPROCESSING)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для 'tweet_stemmed' и 'tweet_lemmatized' создадим мешок слов с помощью CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_bov_countvectorizer(name_column:str, max_features:int=1000, max_df:float=0.9, ngram_range=(1,1)):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=max_df,\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words='english'\n",
    "        )\n",
    "    bow = vectorizer.fit_transform(list(map(lambda text: ' '.join(text), df[name_column])))\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    #print(pd.DataFrame(bow.toarray(), columns = feature_names).head(5))   \n",
    "    return vectorizer, bow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_stemmed, bow_cov_stemmed = create_bov_countvectorizer(TWEET_STEMMED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_lemmatized, bow_cov_lemmatized = create_bov_countvectorizer(TWEET_LEMMATIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для 'tweet_stemmed' и 'tweet_lemmatized' создадим мешок слов с помощью TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def create_bov_tfidfvectorizer(name_column:str, max_features:int=1000, max_df:float=0.9, ngram_range=(1,1)):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=max_df,\n",
    "        max_features=max_features,\n",
    "        ngram_range = ngram_range,\n",
    "        stop_words='english'\n",
    "        )\n",
    "    bow = vectorizer.fit_transform(list(map(lambda text: ' '.join(text), df[name_column])))\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    #print(pd.DataFrame(bow.toarray(), columns = feature_names).head(5))\n",
    "    return vectorizer, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_stemmed, bow_tfidf_stemmed = create_bov_tfidfvectorizer(TWEET_STEMMED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_lemmatized, bow_tfidf_lemmatized = create_bov_tfidfvectorizer(TWEET_LEMMATIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим векторайзеры на корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем данные\n",
    "data = open('hw2/corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# создаем df\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "trainDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def fit_model(vectorizer, name:str, use_svd:bool = False):\n",
    "    xtrain_vectorizing = vectorizer.transform(x_train)\n",
    "    xvalid_vectorizing = vectorizer.transform(x_valid)\n",
    "    \n",
    "    if use_svd:\n",
    "        svd = TruncatedSVD(n_components=200, random_state=42)\n",
    "        xtrain_vectorizing = svd.fit_transform(xtrain_vectorizing)\n",
    "        xvalid_vectorizing = svd.transform(xvalid_vectorizing)\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    classifier.fit(xtrain_vectorizing, y_train)\n",
    "    y_pred = classifier.predict_proba(xvalid_vectorizing)[:,1]\n",
    "    \n",
    "    metric_manager.apply(name, pd.Series(y_valid), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "fit_model(count_vect, 'lection base')\n",
    "\n",
    "fit_model(cov_stemmed, 'count_vectorizer stemmed base')\n",
    "fit_model(cov_lemmatized, 'count_vectorizer lemmatized base')\n",
    "\n",
    "fit_model(tfidf_stemmed, 'tfidf stemmed base')\n",
    "fit_model(tfidf_lemmatized, 'tfidf lemmatized base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_features=800\n",
    "fit_model(create_bov_countvectorizer(TWEET_STEMMED, max_features=800)[0], 'count_vectorizer stemmed max_features=800')\n",
    "fit_model(create_bov_countvectorizer(TWEET_LEMMATIZED, max_features=800)[0], 'count_vectorizer lemmatized max_features=800')\n",
    "\n",
    "fit_model(create_bov_tfidfvectorizer(TWEET_STEMMED, max_features=800)[0], 'tfidf stemmed max_features=800')\n",
    "fit_model(create_bov_tfidfvectorizer(TWEET_LEMMATIZED, max_features=800)[0], 'tfidf lemmatized max_features=800')\n",
    "\n",
    "#max_features=600\n",
    "fit_model(create_bov_countvectorizer(TWEET_STEMMED, max_features=600)[0], 'count_vectorizer stemmed max_features=600')\n",
    "fit_model(create_bov_countvectorizer(TWEET_LEMMATIZED, max_features=600)[0], 'count_vectorizer lemmatized max_features=600')\n",
    "\n",
    "fit_model(create_bov_tfidfvectorizer(TWEET_STEMMED, max_features=600)[0], 'tfidf stemmed max_features=600')\n",
    "fit_model(create_bov_tfidfvectorizer(TWEET_LEMMATIZED, max_features=600)[0], 'tfidf lemmatized max_features=600')\n",
    "\n",
    "#max_df=0.8\n",
    "fit_model(create_bov_countvectorizer(TWEET_STEMMED, max_df=0.8)[0], 'count_vectorizer stemmed max_df=0.8')\n",
    "fit_model(create_bov_countvectorizer(TWEET_LEMMATIZED, max_df=0.8)[0], 'count_vectorizer lemmatized max_df=0.8')\n",
    "\n",
    "fit_model(create_bov_tfidfvectorizer(TWEET_STEMMED, max_df=0.8)[0], 'tfidf stemmed max_df=0.8')\n",
    "fit_model(create_bov_tfidfvectorizer(TWEET_LEMMATIZED, max_df=0.8)[0], 'tfidf lemmatized max_df=0.8')\n",
    "\n",
    "#ngram_range=(2,2)\n",
    "fit_model(create_bov_countvectorizer(TWEET_STEMMED, ngram_range=(2,2))[0], 'count_vectorizer stemmed ngram_range=(2,2)')\n",
    "fit_model(create_bov_countvectorizer(TWEET_LEMMATIZED, ngram_range=(2,2))[0], 'count_vectorizer lemmatized ngram_range=(2,2)')\n",
    "\n",
    "fit_model(create_bov_tfidfvectorizer(TWEET_STEMMED, ngram_range=(2,2))[0], 'tfidf stemmed ngram_range=(2,2)')\n",
    "fit_model(create_bov_tfidfvectorizer(TWEET_LEMMATIZED, ngram_range=(2,2))[0], 'tfidf lemmatized ngram_range=(2,2)')\n",
    "\n",
    "#ngram_range=(1,2)\n",
    "fit_model(create_bov_countvectorizer(TWEET_STEMMED, ngram_range=(1,2))[0], 'count_vectorizer stemmed ngram_range=(1,2)')\n",
    "fit_model(create_bov_countvectorizer(TWEET_LEMMATIZED, ngram_range=(1,2))[0], 'count_vectorizer lemmatized ngram_range=(1,2)')\n",
    "\n",
    "fit_model(create_bov_tfidfvectorizer(TWEET_STEMMED, ngram_range=(1,2))[0], 'tfidf stemmed ngram_range=(1,2)')\n",
    "fit_model(create_bov_tfidfvectorizer(TWEET_LEMMATIZED, ngram_range=(1,2))[0], 'tfidf lemmatized ngram_range=(1,2)')\n",
    "\n",
    "# SVD\n",
    "fit_model(cov_stemmed, 'count_vectorizer stemmed svd', use_svd=True)\n",
    "fit_model(cov_lemmatized, 'count_vectorizer lemmatized svd', use_svd=True)\n",
    "\n",
    "fit_model(tfidf_stemmed, 'tfidf stemmed svd', use_svd=True)\n",
    "fit_model(tfidf_lemmatized, 'tfidf lemmatized svd', use_svd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name model                                       Threshold    F-Score    Precision    Recall    Accuracy    Roc-AUC    f1-score(macro)\n",
      "---------------------------------------------  -----------  ---------  -----------  --------  ----------  ---------  -----------------\n",
      "lection base                                         0.347      0.859        0.828     0.894       0.854      0.924              0.853\n",
      "count_vectorizer stemmed base                        0.261      0.758        0.647     0.915       0.708      0.816              0.695\n",
      "count_vectorizer lemmatized base                     0.407      0.772        0.73      0.82        0.758      0.838              0.757\n",
      "tfidf stemmed base                                   0.383      0.769        0.694     0.863       0.741      0.826              0.737\n",
      "tfidf lemmatized base                                0.387      0.79         0.716     0.88        0.765      0.851              0.762\n",
      "count_vectorizer stemmed max_features=800            0.354      0.754        0.671     0.861       0.719      0.811              0.714\n",
      "count_vectorizer lemmatized max_features=800         0.451      0.762        0.737     0.788       0.753      0.825              0.753\n",
      "tfidf stemmed max_features=800                       0.399      0.761        0.696     0.841       0.736      0.819              0.734\n",
      "tfidf lemmatized max_features=800                    0.429      0.773        0.733     0.818       0.76       0.835              0.759\n",
      "count_vectorizer stemmed max_features=600            0.388      0.734        0.669     0.813       0.705      0.783              0.702\n",
      "count_vectorizer lemmatized max_features=600         0.337      0.745        0.66      0.856       0.708      0.796              0.701\n",
      "tfidf stemmed max_features=600                       0.386      0.737        0.663     0.829       0.704      0.79               0.699\n",
      "tfidf lemmatized max_features=600                    0.422      0.749        0.704     0.801       0.732      0.804              0.73\n",
      "count_vectorizer stemmed max_df=0.8                  0.261      0.758        0.647     0.915       0.708      0.816              0.695\n",
      "count_vectorizer lemmatized max_df=0.8               0.407      0.772        0.73      0.82        0.758      0.838              0.757\n",
      "tfidf stemmed max_df=0.8                             0.383      0.769        0.694     0.863       0.741      0.826              0.737\n",
      "tfidf lemmatized max_df=0.8                          0.387      0.79         0.716     0.88        0.765      0.851              0.762\n",
      "count_vectorizer stemmed ngram_range=(2,2)           0.149      0.666        0.5       1           0.499      0.52               0.333\n",
      "count_vectorizer lemmatized ngram_range=(2,2)        0.297      0.668        0.504     0.99        0.509      0.543              0.362\n",
      "tfidf stemmed ngram_range=(2,2)                      0.205      0.666        0.5       1           0.499      0.527              0.335\n",
      "tfidf lemmatized ngram_range=(2,2)                   0.826    nan            0         0           0.5        0.545              0.334\n",
      "count_vectorizer stemmed ngram_range=(1,2)           0.274      0.755        0.646     0.908       0.705      0.813              0.693\n",
      "count_vectorizer lemmatized ngram_range=(1,2)        0.279      0.763        0.666     0.894       0.722      0.826              0.714\n",
      "tfidf stemmed ngram_range=(1,2)                      0.399      0.765        0.701     0.843       0.741      0.823              0.739\n",
      "tfidf lemmatized ngram_range=(1,2)                   0.414      0.778        0.725     0.84        0.76       0.839              0.759\n",
      "count_vectorizer stemmed svd                         0.367      0.761        0.675     0.872       0.726      0.817              0.72\n",
      "count_vectorizer lemmatized svd                      0.397      0.777        0.712     0.856       0.755      0.842              0.752\n",
      "tfidf stemmed svd                                    0.373      0.768        0.681     0.879       0.734      0.822              0.728\n",
      "tfidf lemmatized svd                                 0.44       0.787        0.744     0.835       0.774      0.847              0.773\n"
     ]
    }
   ],
   "source": [
    "metric_manager.show_table_report()\n",
    "#metric_manager.show_united_auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выводы:\n",
    "1. Count_vectorizer и Tfidf показали в целом одинаковые результаты незначительно опережая друг друга при повторных обучениях\n",
    "2. Lemmatized всегда выше stemmed но не значительно\n",
    "3. Понижение max_features нелинейно ухудшает точность класификации\n",
    "4. Понижение max_df нелинейно ухудшает точность класификации\n",
    "5. Изучение модели только на словосочетаниях значительно хуже чем анализировать текс по одному слову\n",
    "6. Изучение модели на словосочетаниях и по словам не даёт заметного улучшения класификации\n",
    "7. Используя SVD и сжимая количество фич в 5 раз показатели качества модели падают незначительно "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec2f7d8ee0246392929e16f8aba617d0c676e76b82f1f0a5edb051af53e3d64d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
